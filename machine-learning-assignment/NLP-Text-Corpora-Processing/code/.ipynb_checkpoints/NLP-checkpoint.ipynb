{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Working on File 15365.female.34.indUnk.Cancer.xml\n",
      "Currently Working on File 11253.male.26.Technology.Aquarius.xml\n",
      "Currently Working on File 23166.female.25.indUnk.Virgo.xml\n",
      "Currently Working on File 9289.male.23.Marketing.Taurus.xml\n",
      "Currently Working on File 23191.female.23.Advertising.Taurus.xml\n",
      "Currently Working on File 11762.female.25.Student.Aries.xml\n",
      "Currently Working on File 21828.male.40.Internet.Cancer.xml\n",
      "Currently Working on File 8349.male.24.Consulting.Cancer.xml\n",
      "Currently Working on File 5114.male.25.indUnk.Scorpio.xml\n",
      "Currently Working on File 8173.male.42.indUnk.Capricorn.xml\n",
      "Currently Working on File 17944.female.39.indUnk.Sagittarius.xml\n",
      "Currently Working on File 9470.male.25.Communications-Media.Aries.xml\n",
      "Currently Working on File 24336.male.24.Technology.Leo.xml\n",
      "Currently Working on File 7596.male.26.Internet.Scorpio.xml\n",
      "Currently Working on File 23676.male.33.Technology.Scorpio.xml\n",
      "Completed reading all\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "'''\n",
    "In the xml document the & symbol caused a lot of trouble,\n",
    "so by using the HTMl format of Bs4 for reading the document and then\n",
    "parsing the xml into csv.\n",
    "\n",
    "However, in the initial step the blog is converted into a csv file named text.csv,\n",
    "which contain three columns. The first column is the name of the file \n",
    "from where it is read and the second column is the date of the blog post \n",
    "and the last column is text of the blog post on that particular day\n",
    "\n",
    "The delimiters are not taken care at this stage. They are taken care \n",
    "during word tokenization\n",
    "'''\n",
    "filename = []\n",
    "# post_data = []\n",
    "# date_data = []\n",
    "myFile = open('./text.csv','w')\n",
    "wr = csv.writer(myFile,quoting = csv.QUOTE_ALL)\n",
    "folder = 'BlogAuthorshipDataset'\n",
    "wr.writerow([\"FileName\",\"Date\",\"Post\"])\n",
    "for file_name in os.listdir(folder):\n",
    "    print('Currently Working on File',file_name)\n",
    "    file_data = open(folder + '/' + file_name,'r',encoding='latin-1')\n",
    "    soup = BeautifulSoup(file_data,features='lxml')\n",
    "    posts = soup.find_all('post')\n",
    "    dates = soup.find_all('date')\n",
    "    for post,date in zip(posts,dates):\n",
    "        filename.append(file_name)\n",
    "#         post_data.append(post.get_text())\n",
    "#         date_data.append(date.get_text())\n",
    "        wr.writerow([file_name , date.get_text(),post.get_text()])\n",
    "print('Completed reading all')\n",
    "myFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "dataset = pd.read_csv('text.csv')\n",
    "'''\n",
    "The next for loop is to remove the \\n \\t \\r and other delimiters \n",
    "which are present at the start and the end of the blog article\n",
    "\n",
    "After this delimiters are removed we have used the apply function of \n",
    "pandas to sentence_tokenize the blog article and saved in the sentence.csv\n",
    "file which has structure same as that of text.csv describe above\n",
    "'''\n",
    "for i in range(dataset.shape[0]):\n",
    "    dataset.iloc[i].Post = dataset.iloc[i].Post.strip().lower()\n",
    "temp = dataset.Post.apply(sent_tokenize)\n",
    "dataset.to_csv('sentence.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "dataset = pd.read_csv('text.csv')\n",
    "'''\n",
    "The next for loop is to remove the \\n \\t \\r and other delimiters \n",
    "which are present at the start and the end of the blog article\n",
    "\n",
    "After this delimiters are removed we have used the apply function of \n",
    "pandas to word_tokenize the blog article and saved in the sentence.csv\n",
    "file which has structure same as that of text.csv describe above\n",
    "'''\n",
    "for i in range(dataset.shape[0]):\n",
    "    dataset.iloc[i].Post = dataset.iloc[i].Post.strip().lower()\n",
    "    \n",
    "def punctuation_remover(tokens):\n",
    "    return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "dataset.Post = dataset.Post.apply(word_tokenize)\n",
    "dataset.Post = dataset.Post.apply(punctuation_remover)\n",
    "dataset.to_csv('word.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "dataset = pd.read_csv('text.csv')\n",
    "'''\n",
    "The next for loop is to remove the \\n \\t \\r and other delimiters \n",
    "which are present at the start and the end of the blog article\n",
    "\n",
    "After this delimiters are removed we have used the apply function of \n",
    "pandas to word_tokenize the blog article and saved in the sentence.csv\n",
    "file which has structure same as that of text.csv describe above\n",
    "'''\n",
    "def punctuation_remover(tokens):\n",
    "    return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    dataset.iloc[i].Post = dataset.iloc[i].Post.strip().lower()\n",
    "dataset.Post = dataset.Post.apply(word_tokenize)\n",
    "dataset.Post = dataset.Post.apply(punctuation_remover)\n",
    "dataset.Post = dataset.Post.apply(Counter)\n",
    "dataset.to_csv('frequency.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "dataset = pd.read_csv('text.csv')\n",
    "'''\n",
    "This code is to return only the stop words that are used on a \n",
    "particular blog on a particular day. I have returned only one \n",
    "occurance of stopword per post per date. To return all the \n",
    "occurance the function has to be modified a little as shown below\n",
    "\n",
    "def include_stopwords(tokens):\n",
    "    return [word for word in tokens if word in stop_words]\n",
    "\n",
    "'''\n",
    "def include_stopwords(tokens):\n",
    "    return set([word for word in tokens if word in stop_words])\n",
    "\n",
    "def punctuation_remover(tokens):\n",
    "    return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    dataset.iloc[i].Post = dataset.iloc[i].Post.strip().lower()\n",
    "\n",
    "dataset.Post = dataset.Post.apply(word_tokenize)\n",
    "dataset.Post = dataset.Post.apply(punctuation_remover)\n",
    "dataset.Post = dataset.Post.apply(include_stopwords)\n",
    "dataset.to_csv('stopwords.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "dataset = pd.read_csv('text.csv')\n",
    "'''\n",
    "This code is to return only the non-stopwords that are used on a \n",
    "particular blog on a particular day. \n",
    "\n",
    "'''\n",
    "def avoid_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def punctuation_remover(tokens):\n",
    "    return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    dataset.iloc[i].Post = dataset.iloc[i].Post.strip().lower()\n",
    "\n",
    "dataset.Post = dataset.Post.apply(word_tokenize)\n",
    "dataset.Post = dataset.Post.apply(punctuation_remover)\n",
    "dataset.Post = dataset.Post.apply(avoid_stopwords)\n",
    "dataset.to_csv('nonstopwords.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "dataset = pd.read_csv('text.csv')\n",
    "'''\n",
    "This snippet of code is to apply porter stemming on each non-stopping\n",
    "word\n",
    "'''\n",
    "def apply_porter_stemmer(tokens):\n",
    "    return [porter.stem(word) for word in tokens]\n",
    "\n",
    "def avoid_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def punctuation_remover(tokens):\n",
    "    return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    dataset.iloc[i].Post = dataset.iloc[i].Post.strip().lower()\n",
    "\n",
    "dataset.Post = dataset.Post.apply(word_tokenize)\n",
    "dataset.Post = dataset.Post.apply(punctuation_remover)\n",
    "dataset.Post = dataset.Post.apply(avoid_stopwords)\n",
    "dataset.Post = dataset.Post.apply(apply_porter_stemmer)\n",
    "dataset.to_csv('stems.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "dataset = pd.read_csv('text.csv')\n",
    "'''\n",
    "This snippet of code is to apply test-lemmitization on each \n",
    "non-stopping word\n",
    "'''\n",
    "def text_lemmetization(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def avoid_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def punctuation_remover(tokens):\n",
    "    return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    dataset.iloc[i].Post = dataset.iloc[i].Post.strip().lower()\n",
    "\n",
    "dataset.Post = dataset.Post.apply(word_tokenize)\n",
    "dataset.Post = dataset.Post.apply(punctuation_remover)\n",
    "dataset.Post = dataset.Post.apply(avoid_stopwords)\n",
    "dataset.Post = dataset.Post.apply(text_lemmetization)\n",
    "dataset.to_csv('lemmas.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
